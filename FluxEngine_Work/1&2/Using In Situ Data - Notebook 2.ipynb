{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a518ef",
   "metadata": {},
   "source": [
    "# Using Insitu Data with FluxEngine #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0382b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from netCDF4 import Dataset\n",
    "# Install basemap-data-hires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4e599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data file\n",
    "region_data = pd.read_csv('CarrickRoads.tsv', sep='\\t', index_col=0)\n",
    "# Show small proportion of the data\n",
    "region_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the new Dataframe column and fill with a hold value\n",
    "region_data['Hours_since'] = 'hold value'\n",
    "\n",
    "# Produce a datetime object for the first recording \n",
    "# - the zeros in the line below show it's the first row (index starts at zero)\n",
    "start_date = dt.datetime(region_data.loc[0,'Year'],region_data.loc[0,'Month'],region_data.loc[0,'Day'],\n",
    "                            region_data.loc[0,'Hour'],region_data.loc[0,'Minute'],region_data.loc[0,'Second'])\n",
    "\n",
    "# Loop over all rows in the Dataframe - i.e from 0 to the length of the Dataframe\n",
    "for i in range(0,len(region_data)):\n",
    "    # Get the date time object for the currently indexed recording - indexed by i\n",
    "    future_date = dt.datetime(region_data.loc[i,'Year'],region_data.loc[i,'Month'],region_data.loc[i,'Day'],\n",
    "                              region_data.loc[i,'Hour'],region_data.loc[i,'Minute'],region_data.loc[i,'Second'])\n",
    "    \n",
    "    # Find difference between current datetime and inital datetime\n",
    "    day_diff = future_date - start_date\n",
    "    \n",
    "    # Fill Dataframe column with time difference in seconds (found using .total_seconds()) \n",
    "    # divided by 86400 (proportion of days that have passed)\n",
    "    region_data.loc[i,'Hours_since'] = day_diff.total_seconds()/(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb24cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to 'Datetime' and 'Days_since' columns and show first 5 rows.\n",
    "region_data[['Datetime', 'Hours_since']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96751c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min and max of longitude and latitude\n",
    "region_data.describe().loc[['min','max'],['Lon','Lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(location):\n",
    "    if location == 'CarrickRoads':\n",
    "        lon_min = -5.2\n",
    "        lon_max = -4.9\n",
    "        lat_min = 50.1\n",
    "        lat_max = 50.25\n",
    "        return lon_min, lon_max, lat_min, lat_max\n",
    "    \n",
    "    elif location == 'Agulhas':\n",
    "        lon_min = 19.7\n",
    "        lon_max = 20.2\n",
    "        lat_min = -35.0\n",
    "        lat_max = -34.7\n",
    "        return lon_min, lon_max, lat_min, lat_max\n",
    "    \n",
    "    else:\n",
    "        lon_min = input('Enter minimum longitude (most Westerly): ')\n",
    "        lon_max = input('Enter minimum longitude (most Easterly): ')\n",
    "        lat_min = input('Enter minimum latitude (most Southerly): ')\n",
    "        lat_max = input('Enter maximum latitude (most Northerly): ')\n",
    "        print('\\n\\n')\n",
    "        if (lon_min >= lon_max) or (lat_min >= lat_max):\n",
    "            print('Check if min/max were entered in the correct order (is a min greater than a max?)')\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        \n",
    "        \n",
    "        return float(lon_min), float(lon_max), float(lat_min), float(lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change string to 'CarrickRoads', 'Agulhas' or your own region name (use '_' for spaces)\n",
    "region_name = 'CarrickRoads'\n",
    "# Performs function\n",
    "lon_min, lon_max, lat_min, lat_max = get_coords(region_name)\n",
    "\n",
    "# Note: if you're having problems with the input fields you can uncomment the line below \n",
    "# and  just enter the values instead, but also comment out the line above to avoid confusion.\n",
    "\n",
    "# lon_min, lon_max, lat_min, lat_max = __ , __ , __ , __\n",
    "\n",
    "# Print out current values\n",
    "print('Current values:')\n",
    "print(f'Longitude -> \\t min:{lon_min} \\t max:{lon_max}')\n",
    "print(f'Latitude -> \\t min:{lat_min} \\t max:{lat_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb89bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lon = round((lon_max - lon_min)/0.05) + 1\n",
    "lons = np.linspace(lon_min,lon_max,split_lon)\n",
    "\n",
    "split_lat = round((lat_max - lat_min)/0.05) + 1\n",
    "lats = np.linspace(lat_min,lat_max,split_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbe926",
   "metadata": {},
   "source": [
    "### Adding Satelite Sea Surface Sub-skin Temperature\n",
    "To use FluxEngine we need the sea surface sub-skin temperature, which can be aquired in daily resolution from the ESA CCI datasets (found here: https://resources.marine.copernicus.eu/product-detail/SST_GLO_SST_L4_REP_OBSERVATIONS_010_024/INFORMATION). Under the 'Data Access' tab there are two options 'ESACCI-GLO-SST-L4-REP-OBS-SST' which covers 1981-2016, and 'C3S-GLO-SST-L4-REP-OBS-SST' for 2017 onwards. By far the easiest way to download the data is using FTP (File Transfer Protocol) which opens the file folders remotely on your PC. Some browsers (particularlly Safari on MacOS) don't allow FTP, but this can be worked around by using a different browser, such as Firefox. You should be able to connect using the 'guest' option when it prompts you, but if you can't try creating an account with the CMEMS (https://resources.marine.copernicus.eu/registration-form) and using your login at the prompt.\n",
    "\n",
    "The resolution of the ESA CCI data is 0.05x0.05, which results in 25,920,000 data points where most are not needed. You can use the entire dataset with FluxEngine, but it takes a long time to run and also presents problems viewing the data in Panoply. Therefore the following section of code allows us to view only the data in our desired region, and then we can manually fill this in to our dataframe. \n",
    "\n",
    "Below we first read the netCDF SST file (you can subsitute the file string with your own downloaded ESA CCI SST data when needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49597",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_ds = Dataset('20130903120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.0-v02.0-fv01.0.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61e0b8",
   "metadata": {},
   "source": [
    "Now we want to find the SST values just within region we are interested (i.e the region plotted above). The code below takes our min/max longitudes and latitudes and finds where their array position is in the ESA CCI SST data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding SST array index for min/max longitude\n",
    "lon_min_idx = round((lon_min+180)/0.05)\n",
    "lon_max_idx = round((lon_max+180)/0.05)\n",
    "\n",
    "# Finding SST array index for min/max latitude\n",
    "lat_max_idx = round(-(lat_max-90)/0.05)\n",
    "lat_min_idx = round(-(lat_min-90)/0.05)\n",
    "\n",
    "# Print the results\n",
    "print('Indexes:')\n",
    "print(f'Rows: {lat_max_idx} -> {lat_min_idx}')\n",
    "print(f'Cols: {lon_min_idx} -> {lon_max_idx}')\n",
    "print(f'Data slice used: [{lat_max_idx}:{lat_min_idx},{lon_min_idx}:{lon_max_idx}]')\n",
    "\n",
    "# Get ESA SST values by removing the mask - fill land areas with NaN values\n",
    "sst_grid = np.ma.filled(sst_ds.variables['analysed_sst'][:],fill_value=np.nan)\n",
    "sst_grid = np.squeeze(sst_grid) # Reduces data to 2 dimensions\n",
    "sst_grid = np.flip(sst_grid, axis=0) # Flip data (ESA grid is 'upside down')\n",
    "# Reduce data to desired region using the indexes found above\n",
    "region_sst_grid = sst_grid[lat_max_idx:lat_min_idx,lon_min_idx:lon_max_idx]\n",
    "\n",
    "# Plot the region SST data and annotate values (note: annotation not advised for large data sets)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(region_sst_grid,annot=True,fmt=\".3f\",linewidths=1,linecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308a112",
   "metadata": {},
   "source": [
    "In order to not calculate fluxes for the entire globe we need to add the ESA SST data into our region dataset. This can be done prgrammatically or using software like Excel. The big problem with using Excel is that when you import your data as a spreadsheet, Excel will change the format of the datetime column - but we need it as yyyy-mm-dd hh:mm:ss for use with FluxEngine - and then we have to go through the convoluted (but possible) process of changing the format back. Instead there is some code below which extracts the array position of the recorded data, matches this to the region SST grid, and fills in the values for you. This code can be complex to write initially, but will likely save time and effort in the long run. Try running the code below using the example datasets, and then test your own.\n",
    "\n",
    "First we edit our region dataframe to include the 'ESA_SST' column and fill it with a holding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47434bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ESA_SST column and fill with 'hold value'\n",
    "region_data['ESA_SST'] = 'hold value'\n",
    "# Display small section of dataframe\n",
    "region_data[['Datetime','ESA_SST']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the order of the latitudes (matches latitude index with SST array index)\n",
    "flip_lats = np.flip(lats,axis=0)\n",
    "\n",
    "# Loop over rows in region dataframe\n",
    "for r,row in enumerate(region_data.iterrows()):\n",
    "    lat = row[1]['Lat'] # Get latitude in row\n",
    "    lon = row[1]['Lon'] # Get longitude in row\n",
    "    \n",
    "    # Loop over increasing longitude values\n",
    "    # When our 'lon' is less than the longitude we are looping over we know the array index\n",
    "    for lx in range(1,len(lons),1):\n",
    "        if lon < lons[lx]:\n",
    "            break # Breaks out of loop to keep 'lx' = array index\n",
    "            \n",
    "    # Loop over increasing latitude values\n",
    "    # When our 'lat' is less than the latitude we are looping over we know the array index      \n",
    "    for ly in range(len(flip_lats)-2,-1,-1):\n",
    "        if lat < flip_lats[ly]:\n",
    "            break # Breaks out of loop to keep 'ly' = array index\n",
    "    \n",
    "    # In the current row we changes the ESA_SST hold value to the correct value in the SST array\n",
    "    region_data.loc[r,'ESA_SST'] = region_sst_grid[ly,lx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of top of data (early in datetime)\n",
    "region_data[['Lat','Lon','Datetime','ESA_SST']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of bottom of data (late in datetime)\n",
    "region_data[['Lat','Lon','Datetime','ESA_SST']].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa0928",
   "metadata": {},
   "source": [
    "Finally we need to export our new dataframe as a file to be used in FluxEngine (change the file name to suit the dataset you are using) - be careful NOT to overwrite your orginal data just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports data to a TSV - automatically takes name from the one chosen previously\n",
    "region_data.to_csv(f'{region_name}_withESASST.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe158ba8",
   "metadata": {},
   "source": [
    "### Using FluxEngine\n",
    "Now we have plotted our data we can use FluxEngine to export the data to netCDF files and run FluxEngine (as in tutorial 02).\n",
    "\n",
    "Note: don't forget the -h label if you need a reminder of how the commands work\n",
    "\n",
    "Due to how terminal commands are executed this has to be filled out by hand. The commands for the example dataset have been provided (and I suggest looking through all the included commands), but you will need to write your own (or edit the examples) for your own data.\n",
    "\n",
    "When using the example datasets you will need to change the input file (first string), -ncOutPath, and the --limits commands. When using your own datasets you will need to check all command inputs, and change values where needed.\n",
    "\n",
    "(Please also check your datetime format as anything except yyyy-mm-dd hh:mm:ss may cause issues - defintely double check the format if you've used Excel at any point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays values needed for --limit command below\n",
    "print(f'These are the values for the limit commands -> South:{lat_min}, North:{lat_max}, West:{lon_min}, East:{lon_max}')\n",
    "print(f'e.g. --limits {lat_min} {lat_max} {lon_min} {lon_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We primarily use FluxEngine from the command line, but here we can import it just to check the version\n",
    "import fluxengine as fe\n",
    "print(fe.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de293d1d",
   "metadata": {},
   "source": [
    "The following cell converts our text file to netCDF for use with FluxEngine.\n",
    "If changing location to the Agulhas (or your own region) ensure to change the following:\n",
    "- Text file to read (first argument)\n",
    "- Coordinates (in order of South, North, West, East) given after --limits\n",
    "- File name to write out netCDF to given after --ncOutPath\n",
    "- If using your own data may need to change start and end times too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to netCDF\n",
    "!fe_text2ncdf.py \"CarrickRoads_withESASST.tsv\" --limits 50.1 50.25 -5.2 -4.9 --ncOutPath \"CarrickRoads.nc\" --startTime \"2013-09-03 00:00:00\" --endTime \"2013-09-03 23:59:59\" --temporalResolution \"30 00:00\" --cols \"SST_C\" \"windspeed\" \"wind_moment2\" \"air_pressure\" \"salinity\" \"xCO2air\" \"fCO2\" \"ESA_SST\" --dateIndex 3 --latProd \"Lat\" --lonProd \"Lon\" --latResolution 0.005 --lonResolution 0.005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456d7ae",
   "metadata": {},
   "source": [
    "Here we load the configuration file and tell FluxEngine our start and end dates. Change config file if using a different region, and change dates if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266608c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running FluxEngine\n",
    "!fe_run.py \"CarrickRoads.conf\" -s \"2013-09-01\" -e \"2013-09-30\" -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2237b09",
   "metadata": {},
   "source": [
    "Finally we can merge our FluxEngine output into our text file. Depending on region, you may need to change the first 3 arguments:\n",
    "- 1st: FluxEngine output file\n",
    "- 2nd: Text file to merge to\n",
    "- 3rd: New file to create with merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81625c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending FluxEngine results\n",
    "!fe_append2insitu.py \"fe_output/carrickroads/2013/09/carrickroads_fe.nc\" \"CarrickRoads.tsv\" \"CarrickRoads_merged.tsv\" --varsToAppend \"OF\" \"OK3\" \"OSFC\" \"OIC1\" --dateIndex 3 --lonCol \"Lon\" --latCol \"Lat\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791577bb",
   "metadata": {},
   "source": [
    "Now we have run FluxEngine and combine our output with our in-situ data, we can go ahead and view/visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96221dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged data\n",
    "merged_data = pd.read_csv('CarrickRoads_merged.tsv', sep='\\t',index_col=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top of merged data\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820d142",
   "metadata": {},
   "source": [
    "We need to add out 'Days_since' to this new merged dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the new Dataframe column and fill with a hold value\n",
    "merged_data['Hours_since'] = 'hold value'\n",
    "\n",
    "# Produce a datetime object for the first recording \n",
    "# - the zeros in the line below show it's the first row (index starts at zero)\n",
    "start_date = dt.datetime(merged_data.loc[0,'Year'],merged_data.loc[0,'Month'],merged_data.loc[0,'Day'],\n",
    "                            merged_data.loc[0,'Hour'],merged_data.loc[0,'Minute'],merged_data.loc[0,'Second'])\n",
    "\n",
    "# Loop over all rows in the Dataframe - i.e from 0 to the length of the Dataframe\n",
    "for i in range(0,len(merged_data)):\n",
    "    # Get the date time object for the currently indexed recording - indexed by i\n",
    "    future_date = dt.datetime(merged_data.loc[i,'Year'],merged_data.loc[i,'Month'],merged_data.loc[i,'Day'],\n",
    "                              merged_data.loc[i,'Hour'],merged_data.loc[i,'Minute'],merged_data.loc[i,'Second'])\n",
    "    \n",
    "    # Find difference between current datetime and inital datetime\n",
    "    day_diff = future_date - start_date\n",
    "    \n",
    "    # Fill Dataframe column with time difference in seconds (found using .total_seconds()) \n",
    "    # divided by 86400 (proportion of days that have passed)\n",
    "    merged_data.loc[i,'Hours_since'] = day_diff.total_seconds()/(60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92801b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show section of 'Days_since' column for visual check\n",
    "merged_data[['Datetime', 'Hours_since']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918720d",
   "metadata": {},
   "source": [
    "### Plot the Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e132519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with 4 axes on it. Sharex=True means all axes will share the bottom axes (can help with clarity)\n",
    "fig,ax = plt.subplots(1,1, sharex=True, figsize=(20,10))\n",
    "sns.lineplot(data=merged_data, x='Hours_since', y='OF [g C m-2 day-1]')\n",
    "\n",
    "# Plot features \n",
    "plt.xlabel(f'Hours since {merged_data.loc[0,\"Datetime\"]}', fontdict={'size':20})\n",
    "plt.ylabel('Flux [g C m-2 day-1]', fontdict={'size':20})\n",
    "plt.tick_params(labelsize=15)\n",
    "\n",
    "# Show figure!\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
